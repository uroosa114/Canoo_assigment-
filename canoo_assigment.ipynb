{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoHgkVwO68lzvLTw6U/Z9u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uroosa114/Canoo_assigment-/blob/main/canoo_assigment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckduckgo_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18g22CkUcp3k",
        "outputId": "32a5879d-a34b-41f3-8dd9-74987f4e8d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting duckduckgo_search\n",
            "  Downloading duckduckgo_search-4.4.3-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo_search) (8.1.7)\n",
            "Collecting curl-cffi>=0.6.0b9 (from duckduckgo_search)\n",
            "  Downloading curl_cffi-0.6.0b9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lxml>=5.1.0 (from duckduckgo_search)\n",
            "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from duckduckgo_search) (1.6.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from curl-cffi>=0.6.0b9->duckduckgo_search) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from curl-cffi>=0.6.0b9->duckduckgo_search) (2024.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12.0->curl-cffi>=0.6.0b9->duckduckgo_search) (2.21)\n",
            "Installing collected packages: lxml, curl-cffi, duckduckgo_search\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.9.4\n",
            "    Uninstalling lxml-4.9.4:\n",
            "      Successfully uninstalled lxml-4.9.4\n",
            "Successfully installed curl-cffi-0.6.0b9 duckduckgo_search-4.4.3 lxml-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from duckduckgo_search import DDGS\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Search queries\n",
        "queries = [\n",
        "    \"Identify the industry in which Canoo operates, along with its size, growth rate, trends, and key players\",\n",
        "    \"Analyze Canoo's main competitors, including their market share, products or services offered, pricing strategies, and marketing efforts\",\n",
        "    \"Identify key trends in the market, including changes in consumer behavior, technological advancements, and shifts in the competitive landscape\",\n",
        "    \"Gather information on Canoo's financial performance, including its revenue, profit margins, return on investment, and expense structure.\"\n",
        "]\n",
        "\n",
        "# Initialize an empty DataFrame to store the query results\n",
        "query_results_df = pd.DataFrame()\n",
        "\n",
        "# Perform search and store results in DataFrame\n",
        "for query in queries:\n",
        "    with DDGS() as ddgs:\n",
        "        results = [{'title': r['title'], 'url': r['href']} for r in ddgs.text(query, max_results=10)]\n",
        "        df = pd.DataFrame(results)\n",
        "        query_results_df = pd.concat([query_results_df, df], ignore_index=True)\n",
        "\n",
        "# Export the query results DataFrame to a CSV file\n",
        "query_results_df.to_csv('query_results.csv', index=False)\n",
        "\n",
        "# Initialize lists to store scraped data\n",
        "urls = []\n",
        "texts = []\n",
        "titles = []\n",
        "\n",
        "# Asynchronous scraping function\n",
        "async def save_content(title, url, text):\n",
        "    words = text.split()\n",
        "    truncated_text = ' '.join(words[:2000])\n",
        "    titles.append(title)\n",
        "    urls.append(url)\n",
        "    texts.append(truncated_text)\n",
        "\n",
        "async def scrape_content(title, url):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.get(url) as resp:\n",
        "            if resp.status == 200:\n",
        "                body = await resp.text()\n",
        "                soup = BeautifulSoup(body, 'html.parser')\n",
        "                content = soup.find_all(['p', 'span'])\n",
        "                text = ' '.join([p.get_text().strip() for p in content])\n",
        "            else:\n",
        "                text = ''\n",
        "        await save_content(title, url, text)\n",
        "\n",
        "# Main asynchronous function\n",
        "async def main():\n",
        "    tasks = []\n",
        "    # Read query results CSV and initiate scraping tasks\n",
        "    with open('query_results.csv') as file:\n",
        "        csv_reader = csv.DictReader(file)\n",
        "        for csv_row in csv_reader:\n",
        "            task = asyncio.create_task(scrape_content(csv_row['title'], csv_row['url']))\n",
        "            tasks.append(task)\n",
        "\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "    # Create DataFrame from scraped data and export to CSV\n",
        "    result_df = pd.DataFrame({'Title': titles, 'URL': urls, 'Text': texts})\n",
        "    result_df.to_csv('scraped_data.csv', index=False)\n",
        "\n",
        "# Run the main asynchronous function\n",
        "asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "Ibm9nutAel3U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}